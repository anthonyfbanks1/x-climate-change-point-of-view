Imports
#Import dependencies
import os
import hvplot.pandas

import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

from scipy.optimize import curve_fit
from scipy.stats import linregress
from citipy import citipy
from datetime import datetime

# Remove warnings for saving map file
import shapely
import warnings
from shapely.errors import ShapelyDeprecationWarning
warnings.filterwarnings("ignore", category=ShapelyDeprecationWarning) 
Question 1: Exploring Trends in North America
Are there any trends in sentiment, aggression, or stance over time in North America? Are there any trends that coincide with current events?

# Import Data
path = os.path.join("Resources", "NA_sample_020.csv")
df = pd.read_csv(path)
df.head()
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness
0	8037902	2018-01-31 20:40:44+00:00	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive
1	8790126	2018-03-15 21:28:05+00:00	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive
2	11647260	2018-09-29 14:42:56+00:00	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive
3	11810796	2018-10-08 22:23:50+00:00	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive
4	13183104	2018-12-16 11:52:40+00:00	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive
# Create new df with column with month and date
dates = pd.to_datetime(df["created_at"], infer_datetime_format=True)    #Make variable where the created_at column is converted to datetime
df_month = df.loc[:, ["created_at", "lat", "lng", "sentiment", "topic", "stance", "gender", "aggressiveness"]]  # Make smaller DF with just a few of the columms
df_month["created_at"] = dates  # Change the "created_at" column to the converted date time values
df_month['date'] = df_month['created_at'].dt.to_period('M') # Make a new column where the month and year are extracted from the dates

#Add a column for the year
df_month['year'] = df_month['created_at'].dt.year

df_month.head()
c:\Users\barim\anaconda3\envs\PythonData\lib\site-packages\pandas\core\arrays\datetimes.py:1146: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.
  UserWarning,
created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date	year
0	2018-01-31 20:40:44+00:00	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive	2018-01	2018
1	2018-03-15 21:28:05+00:00	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive	2018-03	2018
2	2018-09-29 14:42:56+00:00	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive	2018-09	2018
3	2018-10-08 22:23:50+00:00	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive	2018-10	2018
4	2018-12-16 11:52:40+00:00	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive	2018-12	2018
Looking at Stance, Aggression, and Sentiment Over Time
#Make Functions to assign numerical value for stance and aggression
def get_agg_factor(x):
    if x == 'aggressive':
        return 100.0
    else:
        return 0.0

def get_st_factor(x):
    if x == 'believer':
        return 100.0
    else:
        return 0.0

df_month["aggressiveness_factor"] = df_month['aggressiveness'].apply(lambda x: get_agg_factor(x))   #Make new column for numerical values for aggression
df_month["stance_factor"] = df_month['stance'].apply(lambda x: get_st_factor(x))    # Make new column for numerical values for stance

df_month.head()
created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date	year	aggressiveness_factor	stance_factor
0	2018-01-31 20:40:44+00:00	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive	2018-01	2018	100.0	100.0
1	2018-03-15 21:28:05+00:00	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive	2018-03	2018	0.0	100.0
2	2018-09-29 14:42:56+00:00	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive	2018-09	2018	0.0	100.0
3	2018-10-08 22:23:50+00:00	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive	2018-10	2018	0.0	100.0
4	2018-12-16 11:52:40+00:00	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive	2018-12	2018	0.0	100.0
# Change the type for the factors to be floats
df_month = df_month.astype({
    "aggressiveness_factor": "float64",
    "stance_factor": "float64"
})

# Take the Average Sentiment, aggressiveness, and stance for each month using Groupby
df_averages = df_month.groupby("date")[["aggressiveness_factor", "stance_factor", "sentiment"]].mean() 
# The aggressiveness and stance factors give the percentages of people who are aggressive or believe in climate change
Plot Sentiment, Aggressiveness, and Stance Over Time
#Plot 
axes = df_averages.plot(subplots=True, figsize=(10, 10))

# For each plot, make a vertical line for dates where it looks like there is a change in the trend
axes[0].set_ylabel("Aggressive Tweets %")
axes[0].axvline(pd.to_datetime("2014-03-01"), color='r', linestyle='--', label = "Apr 2014")
axes[0].axvline(pd.to_datetime("2015-11-01"), color='k', linestyle='--', label = "~2016 Election Kickoff")
axes[0].axvline(pd.to_datetime("2017-08-01"), color='m', linestyle='--', label = "Sep 2017")
axes[0].legend(loc = "upper left")


axes[1].set_ylabel("Believer Tweets %")
axes[1].axvline(pd.to_datetime("2014-03-01"), color='r', linestyle='--', label = "Apr 2014")
axes[1].axvline(pd.to_datetime("2015-11-01"), color='k', linestyle='--', label = "~2016 Election Kickoff")
axes[1].axvline(pd.to_datetime("2017-08-01"), color='m', linestyle='--', label = "Sep 2017")
axes[1].legend(loc = "upper left")

axes[2].set_ylabel("Average Sentiment (from -1 to 1)")
axes[2].axvline(pd.to_datetime("2014-03-01"), color='r', linestyle='--', label = "Apr 2014")
axes[2].axvline(pd.to_datetime("2015-11-01"), color='k', linestyle='--', label = "~2016 Election Kickoff")
axes[2].axvline(pd.to_datetime("2017-08-01"), color='m', linestyle='--', label = "Sep 2017")
axes[2].legend(loc = "best")

axes[0].set_title("Sentiment, Aggression, and Stance Over Time")

plt.tight_layout()

# Save plot
plt.savefig("Images/Q1_Trends.png", transparent = False, facecolor= "white")

Analysis
The percent of aggressive tweets is gradually decreasing over time. The percentage of tweets that support the belief of climate change is increasing overall. The average sentiment for each month fluctuates and contains smaller trends.

Before early 2014, there is higher variability in the data and any trends present. Starting in early 2014, some clear and more steady and positive trends appear in the percentage of tweets that support climate change and that have positive sentiment. Separate research shows that in 2014, the world experienced many climate related natural disaster starting with wildfires in early 2014 in Australia and heat waves in Canada and Alaska.

Starting in late 2015, the sentiment switches from steadily increasing, to steadily decreasing. The percentage of tweets supporting climate change stops increasing and begins to decrease. The percent of aggressive tweets stops decreasing, and increases slight. These trends end in mid 2017. The start of these trends coincides with the ramping up of the 2016 election cycle which had climate change as a major devicisve talking point/issue. The trend ends in mid-late 2017. A notible climate change event that may have had an import was the US leaving the Paris Climate Agreement in June 2017.

A large jump in the average sentiment and the percent of tweet that believe in climate change occurs in the Fall of 2017, after which the data becomes more variable. The percentage of aggressive tweets begins to decrease again.

Comparing the Number of Tweets that Support, are Neutral on, or Deny Climate Change
# Create a df where the data is grouped by the year and the stance of the tweet. Count the number of tweets in each category
stance_df = df_month.groupby(["year", "stance"])["gender"].count()

fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))

stance_df.unstack().plot(kind='bar', stacked=True, ax=axes[0]) # make one plot have a linear y axis
stance_df.unstack().plot(kind = "bar", stacked = True, ax=axes[1], logy= True)  # make a plot with a logarithmic y axis

axes[0].set_xlabel("Year")
axes[0].set_ylabel("# of Tweets")

axes[1].set_xlabel("Year")
axes[1].set_ylabel("# of Tweets (Log Scale)")



plt.tight_layout()

# Save plot
plt.savefig("Images/Q1_StanceNumbers.png", transparent = False, facecolor= "white")

Analysis:
The number of tweets about climate change decreases from 2019 to 2012. After 2012 there is a rapid increase in the number of tweets per year about climate change. The increase appears exponential. The increase in the number of tweets could be due to an increase in the numner of consumers joining Twitter or the increased politcal attention to climate change starting in 2015/2016 in the 2016 election cycle. A rapid increase could also be related to Greta Thunberg entering the international stage in 2018 and garnering a large Twitter following.

The second graph was made with the same data but uses a logarithmic scale to better examine how the number of believer, neutral, and deniers tweets changed over time. It is clear that the changes are explonential. It is less clear how the number of believer tweets is changing compared to the number of denier tweets.

Taking a Closer Look at Stance Over Time
Exponential Regression for Believers
# Make a data frame with just stance
series_stance = df_averages["stance_factor"]
df_stance = series_stance.to_frame()    # Convert the series back into a dataframe
df_stance.reset_index(inplace=True) # Reset the index so that Date is a column

def change_timestamp(x):    #Made function to change period to timestamp
    return x.to_timestamp()

df_stance["date"] = df_stance['date'].apply(lambda x: change_timestamp(x))      #Change all date column to timestamp
# Just look at the time after 2012 and before 2019
# Make a new dataframe looking at just after 2012 and cutting off the outlier of October 2019
df_2012 = df_stance.loc[(df_stance["date"] >= (pd.to_datetime("2012-02-01"))) & (df_stance["date"] <= (pd.to_datetime("2019-09-01"))), :]
x_values = [x for x in range(len(df_2012))] # Make a list of integers to represent the number of months since February 2012 to make it easier to conduct exponential regressions
y = list(df_2012["stance_factor"])  # Make the dependent variable the stances (the percent of believer tweets)
y_values = [y for y in y]   # Make the y values just a list 
len(y_values)
92
# Define the function that the data will be fit to
def func_exp(x, a, b, c):
        return a * np.exp(b * x) + c

# Conver the variables into arrays
x_data = np.array(x_values) 
y_data = np.array(y_values)

# Regression! 
popt, pcov = curve_fit(func_exp, x_data, y_data, p0 = (-1, 0.01, 1))

# Write the equation for Believers! 
expo_eq = "y = " + str(round(popt[0],2)) + "e^("+ str(round(popt[1],5)) + "x) + " + str(round(popt[2],2))  
Exponential Regression for Deniers
# CMake function to assign numerical value for stance where being denier is 100
df_deniers = df_month.copy()

def get_denier_factor(x):
    if x == 'denier':
        return 100.0
    else:
        return 0.0

df_deniers["denier_factor"] = df_deniers['stance'].apply(lambda x: get_denier_factor(x))    # make column with numerical value for stance

df_deniers.head()
created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date	year	aggressiveness_factor	stance_factor	denier_factor
0	2018-01-31 20:40:44+00:00	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive	2018-01	2018	100.0	100.0	0.0
1	2018-03-15 21:28:05+00:00	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive	2018-03	2018	0.0	100.0	0.0
2	2018-09-29 14:42:56+00:00	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive	2018-09	2018	0.0	100.0	0.0
3	2018-10-08 22:23:50+00:00	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive	2018-10	2018	0.0	100.0	0.0
4	2018-12-16 11:52:40+00:00	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive	2018-12	2018	0.0	100.0	0.0
# Make a df of the average percentage of denier tweets for each month
df_avdeniers = df_deniers.groupby("date")["denier_factor"].mean() 

# Make sure that there is a date column converted to datetime
df_den = df_avdeniers.to_frame()
df_den.reset_index(inplace=True)
df_den["date"] = df_den['date'].apply(lambda x: change_timestamp(x))  

# Make a new dataframe looking at just after 2012 and cutting off the outlier of October 2019
df_2012_den = df_den.loc[(df_den["date"] >= (pd.to_datetime("2012-02-01"))) & (df_den["date"] <= (pd.to_datetime("2019-09-01"))), :]
# Define x and y axis variables.
x_values_2 = [x for x in range(len(df_2012_den))]   # Make a list of integers instead of time so it is easier to perform regression
y_2 = list(df_2012_den["denier_factor"])
y_values_2 = [y for y in y_2]
# Define the function that the data will be fit to
def func_exp(x, a, b, c):
        return a * np.exp(b * x) + c

# Put variables into arrrays 
x_data_2 = np.array(x_values_2) 
y_data_2 = np.array(y_values_2)

#Regression! 
popt_2, pcov_2 = curve_fit(func_exp, x_data_2, y_data_2, p0 = (-1, 0.001, 1))

# Make equation for denier regression
expo_eq_2 = "y = " + str(round(popt_2[0],2)) + "e^("+ str(round(popt_2[1],5)) + "x) + " + str(round(popt_2[2],2))  
print(popt_2)
[ 2.56180367e+01 -5.62964405e-03 -8.41142972e+00]
Plot Both Regressions Together
# Plot using subplots 
regression_stance, axs = plt.subplots(2, 1, figsize=(6, 8), sharey=True)

axs[0].scatter(x_data, y_data)
axs[0].plot(x_data, func_exp(x_data, *popt), color = "red")
axs[0].annotate(expo_eq,(25, 12),fontsize=15,color="red") # Graphing the  equation on the plot 
axs[0].set_ylabel("'%' of Tweets from Believers")

axs[1].scatter(x_data_2, y_data_2)
axs[1].plot(x_data_2, func_exp(x_data_2, *popt_2), color = "red")
axs[1].set_ylabel("'%' of Tweets from Deniers")
axs[1].set_xlabel("Number of Months")
axs[1].annotate(expo_eq_2,(25,42),fontsize=15,color="red") # Graphing the  equation on the plot 



regression_stance.suptitle('Percent of Tweets from Believers and Deniers Over Time')
regression_stance.tight_layout()


plt.tight_layout()

# Save plot
plt.savefig("Images/Q1_Stance_Regression.png", transparent = False, facecolor= "white")

Analysis:
There is growth in the percentage of believer tweets, but the rate is slowing down. There is a decrease in the percentage of tweets that deny climate change.

If these trends continue, the percent of tweets from believers will reach 100% in 12-15 years, and the percent of denier tweets will reach 0% in 7-8 years. The descrepancy in time could be the neutral tweets. However, are these results due to deniers changing their minds or are deniers leaving Twitter? Or are the number of people joining Twitter who believe in climate change and are active dwarf the number of deniers joning/being active on Twitter?

Question 2: Exploring Trends in the Spread of Misinformation
What trends might we uncover by isolating the tweets denying climate change? Are there any patterns to find that would help make sense of the spread of misinformation on social media? What connection is there (if any) between climate change denial and negative sentiment in Twitter posts on this subject?

# Read csv file

deniers = os.path.join('Resources', 'Climate_Deniers.csv')
deniers_data = pd.read_csv(deniers)

# Convert to appropriate data types & extract date column

deniers_data['created_at'] = deniers_data['created_at'].astype('datetime64')
deniers_data['date'] = deniers_data['created_at'].dt.to_period('M')

# Organize columns & display data

denier_twts = deniers_data.loc[:, ['date', 'lat', 'lng', 'topic', 'sentiment', 'aggressiveness', 'gender', 'created_at']]
print(denier_twts.dtypes)
denier_twts.head()
date                   period[M]
lat                      float64
lng                      float64
topic                     object
sentiment                float64
aggressiveness            object
gender                    object
created_at        datetime64[ns]
dtype: object
date	lat	lng	topic	sentiment	aggressiveness	gender	created_at
0	2006-12	36.072640	-79.791980	Weather Extremes	-0.565028	aggressive	male	2006-12-17 19:43:09
1	2007-01	36.112637	-80.014484	Weather Extremes	-0.377974	aggressive	male	2007-01-18 13:22:52
2	2007-01	25.963890	-80.244170	Weather Extremes	0.567071	not aggressive	undefined	2007-01-24 10:13:18
3	2007-03	30.287986	-97.778898	Weather Extremes	-0.253728	not aggressive	male	2007-03-03 15:23:53
4	2007-03	37.779026	-122.419906	Weather Extremes	-0.433604	aggressive	male	2007-03-13 23:03:07
Geographic Trends in Early Climate Denier Tweets:
# Group by topics & get the first few posts for each

all_topics = denier_twts.groupby('topic')['created_at'].nsmallest(5)

# Merge this set with original df to get the info for each post

use_this = pd.merge(all_topics, denier_twts, how='left')


""" If you would like to see only one topic on the below map: change the string in the 
following code:
Topic options include:
    Donald Trump versus Science, Global stance, Weather Extremes,
    Ideological Positions on Global Warming, Impact of Resource Overconsumption, 
    Importance of Human Intervantion, Politics, Seriousness of Gas Emissions, 
    Significance of Pollution Awareness Events, Undefined / One Word Hashtags """

one_topic = use_this[use_this['topic'] == "Donald Trump versus Science"]  # <-- Add topic here

""" uncomment the next code line to activate above topic instead of all topics """

#use_this = one_topic.copy()


# Get nearest city to each tweet for context

use_this['city'] = ""
for index, row in use_this.iterrows():
    use_this.loc[index, 'city'] = citipy.nearest_city(row['lat'], row['lng']).city_name


# Pull the first tweet in each group to see where the conversation originated

frst = use_this.groupby('topic')['created_at'].min()
first_twts = pd.merge(frst, use_this, how='left')

# Display results

print(len(use_this))
srt = use_this.sort_values('topic', ascending=False)
first_twts
50
created_at	date	lat	lng	topic	sentiment	aggressiveness	gender	city
0	2007-04-06 08:17:31	2007-04	35.227209	-80.843083	Donald Trump versus Science	-0.604242	not aggressive	male	charlotte
1	2007-03-31 12:16:59	2007-03	35.227090	-80.843130	Global stance	-0.404815	not aggressive	male	charlotte
2	2007-04-07 15:26:24	2007-04	30.271129	-97.743700	Ideological Positions on Global Warming	-0.515978	not aggressive	male	austin
3	2007-05-02 17:11:53	2007-05	50.666480	-120.319200	Impact of Resource Overconsumption	-0.690117	aggressive	male	kamloops
4	2007-04-20 18:17:23	2007-04	42.081156	-87.980216	Importance of Human Intervantion	-0.359227	aggressive	male	arlington heights
5	2007-08-19 13:33:24	2007-08	51.508530	-0.125740	Politics	-0.295054	aggressive	male	london
6	2007-04-12 03:40:27	2007-04	27.947520	-82.458430	Seriousness of Gas Emissions	-0.456556	aggressive	male	tampa
7	2007-10-03 06:57:04	2007-10	-27.467940	153.028090	Significance of Pollution Awareness Events	-0.737047	aggressive	male	brisbane
8	2007-07-12 17:49:09	2007-07	38.895110	-77.036370	Undefined / One Word Hashtags	0.761608	not aggressive	male	washington
9	2006-12-17 19:43:09	2006-12	36.072640	-79.791980	Weather Extremes	-0.565028	aggressive	male	greensboro
%%capture --no-display

# Plot points of earliest tweets on each topic

map1 = srt.hvplot.points(
    "lng",
    "lat",
    coastline = True,
    tiles = "OSM",
    frame_width = 550,
    frame_height = 350,
    hover_cols = ["lat", "lng", "topic", "city"],
    s = 90,
    color = "topic",
    alpha = .8,
    title = "Earliest 5 Tweets on Climate Change by Topic/Location"
) 

# Plot a white circle in the middle of the first tweet sent for each topic to highlight them

map2 = first_twts.hvplot.points(
    "lng",
    "lat",
    geo=True,
    frame_width = 550,
    frame_height = 350,
    hover_cols = ["lat", "lng", "topic", "city"],
    s = 15,
    color = "white",
)

# Overlay both dataframes onto the same map plot

full = map1 * map2

# Save copy of map (this usually requires extra downloads to accomplish for most users)
try:
    hvplot.save(full, "Images/Q2_Earliest-Tweets-Map.png")
except Exception as e:
    print(e)

# Display the map

full
Geographic Topic Analysis:
Climate denial has existed since the late ninties, well before Twitter first became publicly available in 2006. From our dataset, we learned that tweets about climate denial found their way onto the platform almost from the beginning, with the first tweet from a climate denier on the subject sent in December of that same year.

The above map shows the geolocation of the first 5 tweets about climate change from the stance of "Denier" on each subtopic as shown in the legend to the right. The locations highlighted with a white circle are the first tweets on each subject, to show the origin point of the conversation.

What we notice is that most of tweets originate in coastal or near-coastal regions. From the above analysis, we can also see that many of these tweets are from large, highly populated cities as well. 7 of the first 10 tweets are from the US, 3 are from the same state of North Carolina, and 2 were sent from the same city of Charlotte.

There doesn't seem to be any major discernible pattern to the spread of the topics geographically, and this isn't very surpising given the medium of social media. It's difficult to determine if this kind of distribution is related to the subject matter of climte change specifically, or to Twitter in general, but it does seem to be an interesting pattern that may merit further study.

Trends in Climate Denier Sentiment/Aggression Over Time:
# Find quartiles of sentiment value

print(denier_twts['sentiment'].describe())

# Create bins to group sentiment by

bins = [-1, -.5, -.2, 0, 1]
names = ["Very Low", "Low", "Average", "High"]

# Cut dataset into sentiment groups by quartiles & display results

denier_twts['Sentiment'] = pd.cut(denier_twts['sentiment'], bins, labels=names, include_lowest=True)
denier_twts.head()
count    365317.000000
mean         -0.225414
std           0.384433
min          -0.989643
25%          -0.522066
50%          -0.331963
75%           0.030237
max           0.979194
Name: sentiment, dtype: float64
date	lat	lng	topic	sentiment	aggressiveness	gender	created_at	Sentiment
0	2006-12	36.072640	-79.791980	Weather Extremes	-0.565028	aggressive	male	2006-12-17 19:43:09	Very Low
1	2007-01	36.112637	-80.014484	Weather Extremes	-0.377974	aggressive	male	2007-01-18 13:22:52	Low
2	2007-01	25.963890	-80.244170	Weather Extremes	0.567071	not aggressive	undefined	2007-01-24 10:13:18	High
3	2007-03	30.287986	-97.778898	Weather Extremes	-0.253728	not aggressive	male	2007-03-03 15:23:53	Low
4	2007-03	37.779026	-122.419906	Weather Extremes	-0.433604	aggressive	male	2007-03-13 23:03:07	Low
# Create 'years' column to group data by for broad overview plot

yrs = denier_twts['date'].dt.year
table = denier_twts.copy()
table['year'] = yrs

# Create function to plot lines for each sentiment bin level

def plot_sent(name, color):
    d1 = table.loc[(table['Sentiment'] == name) & (table['aggressiveness'] == 'aggressive'), :].reset_index(drop=True)
    d2 = table.loc[(table['Sentiment'] == name) & (table['aggressiveness'] != 'aggressive'), :].reset_index(drop=True)
    count = d1.groupby('year').count()
    count2 = d2.groupby('year').count()

    # Plot a solid line for aggressive tweets and dotted line for non-aggressive tweets

    count['topic'].plot(kind='line', color=color, label=name + " - aggressive")
    count2['topic'].plot(kind='line', color=color, linestyle="--", label=name + " - non-aggressive")

# Graph the lines in colors to represent the sentiment levels

plot_sent('High', 'dodgerblue')
plot_sent('Average', 'olive')
plot_sent('Low', 'darkorange')
plot_sent('Very Low', 'crimson')

# Add other graph elements & display results

plt.xlabel("Time of Tweet")
plt.ylabel("Number of Tweets")
plt.title("Denier Tweets by Sentiment/Aggression Level")

plt.axvline(2016, color='purple', linestyle=':')
plt.text(2015.5, 12000, 'Trump campaign begins', color='purple', rotation=90)

plt.legend(loc='best')
plt.tight_layout()
plt.savefig("Images/Q2_Deniers-Sentiment-Overview.png")
plt.show()

Denier Sentiment/Aggression Analysis:
This graph shows the total number of tweets from the stance "Denier" across the timeframe of the full dataset, grouped by sentiment level & aggression. "Sentiment" was a somewhat difficult concept to grasp from the context of the original dataset documentation, but seems to refer to the sentiment of the tweet itself rather than the user's sentiment toward climate change, which is why we see a number of positive sentiment tweets from climate deniers, which would otherwise seem to be an oxymoron. This means that positive sentiment is associated with people communicating with someone who agrees with them, and negative sentiment would be associated with arguments and disagreements.

What we see from the above graph is that both high and low sentiments are consistently favored across the whole timeframe, where average sentiments consistently remain along the bottom and less common. This is what we would expect to see on social media sites, especially during times of high engagement, either strong positive or negative emotions being the norm when people are communicating with others, and a lot less neutral statements.

Another slightly more subtle pattern is that those with "Very Low" sentiment have a tendency to be aggressive more often than they are non-aggressive, particularly when the number of tweets are higher, or during "high engagement" time periods.

The dramatic increase in tweets after the beginning of the 2016 presidential campaign is common to users of all stances, suggesting a boost in popularity in Twitter at that point, or at least in the discussion of the subject of climate change on Twitter, but it is not limited to only deniers of climate change only. This suggests that this graph shows us either the shape of the conversation around climate change, the shape of the popularity of Twitter in general, or some combination of both.

ANOVA: Relationship Between Stance and Sentiment
# Import file containing larger sample to compare deniers to other twitter users

sample5 = os.path.join('Resources', 'sample_05.csv')
frame5 = pd.read_csv(sample5)

# Create new dataframe with stance and sentiment

bx = frame5.loc[:, ['stance', 'sentiment']]
bx.head()
stance	sentiment
0	neutral	0.602393
1	believer	-0.410768
2	believer	0.717759
3	believer	0.306821
4	believer	0.153192
# Get groups of each stance to run ANOVA test & display results

group1 = bx[bx['stance'] == 'believer']['sentiment']
group2 = bx[bx['stance'] == 'neutral']['sentiment']
group3 = bx[bx['stance'] == 'denier']['sentiment']

anova = stats.f_oneway(group1, group2, group3)
anova
F_onewayResult(statistic=9506.935741646466, pvalue=0.0)
# Create boxplots of ANOVA results for visual confirmation

props = dict(medians="red", whiskers="black", caps="black")
boxprops = dict(fc = 'silver')
bx.boxplot('sentiment', by='stance', sym='r.', color=props, boxprops=boxprops, patch_artist=True)

# Add other elements to graph

plt.xlabel('Stance on Climate Change')
plt.ylabel('Tweet Sentiment')
plt.title(f'ANOVA: Stance vs Sentiment\nstatistic={round(anova[0],2)}, p-value={anova[1]}')

# Save file and display results

plt.tight_layout()
plt.savefig('Images/Q2_ANOVA_results.png')
plt.show()

# Calculate IQR for denier tweets to determine range of potential outliers shown on graph

denier_quartiles = group3.quantile([.25, .5, .75])
lower = denier_quartiles[0.25]
upper = denier_quartiles[0.75]
iqr = upper-lower
upper_bnd = upper + (1.5 * iqr)

# Display results

print(f'Denier tweets with sentiment above {upper_bnd} appear as outliers')
Denier tweets with sentiment above 0.8455198454141618 appear as outliers
ANOVA Analysis:
Finally, the above graph shows the results of an ANOVA test to check if there is a relationship between Twitter user stance and sentiment levels. For this test we returned to the original dataset and took a random sampling of 5% of the total worldwide data (due to filesize constraints) in order to compare all stances.

What we see is a clear, significant difference in the mean sentiment of denier posts in comparison to all other posts from both believers and those expressing no stance on the subject of climate change. These latter two both average a neutral sentiment, as would be expected, where the mean of the denier tweets falls much lower. We also found that denier tweets with a sentiment above about 0.85 were likely to be outliers.

While this result is dramatic, it isn't necessarily surprising. Denier tweets were about 8% of the total 15 million data points in the original dataset, and a minority group is likely to spend time debating with people who disagree with them more often than not simply due to being outnumbered. However, it could also imply that deniers are spending more time trying to convert those who disagree with them rather than communicating with those who already agree. Further study would be required to determine more information on that subject, but it is an interesting finding nonetheless.

Question 3: Twitter Response to US Climate Disasters
testfile = 'Resources/disasters.csv'
df = pd.read_csv(testfile)
df.head()
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
0	Earthquake	Ground movement	Natural	Geophysical	NaN	NaN	Japan	Nanao, Wajima districts (Isikawa province), Ni...	37.336	136.588	2007-03-25	2007-03-25	1.0	NaN	NaN	250000.0	81.101659
1	Storm	Tropical cyclone	Natural	Meteorological	Indhala	NaN	Madagascar	Diana, Sava, Sofia, Analanjirofo provinces	-14.840	49.940	2007-03-15	2007-03-17	80.0	203182.0	NaN	240000.0	81.101659
2	Flood	Flash flood	Natural	Hydrological	NaN	Storms and heavy rains	Australia	Gosford, Dungog, Newcastle, Wyong, Port Stephe...	-32.870	151.380	2007-06-08	2007-06-12	9.0	5000.0	NaN	1300000.0	81.101659
3	Flood	Riverine flood	Natural	Hydrological	NaN	Heavy rains	Haiti	Ferrier village (Fort Liberte district, Nord E...	19.410	-71.780	2007-03-26	2007-03-30	14.0	15000.0	NaN	NaN	81.101659
4	Flood	Riverine flood	Natural	Hydrological	NaN	Heavy rains	Argentina	Tucuman, Santiago del Estero, Salta, Formosa p...	NaN	NaN	2007-01-18	2007-03-20	5.0	60000.0	NaN	30000.0	81.101659
Formatting Dataset
# Find Disaster subtype for USA
usa_df = df.loc[(df['Country'] == 'United States of America (the)'),:]
usa_df['Disaster Subtype'].value_counts()
Convective storm                    156
Tropical cyclone                     37
Riverine flood                       34
Forest fire                          19
Flash flood                           8
Drought                               8
Land fire (Brush, Bush, Pasture)      7
Ground movement                       5
Severe winter conditions              2
Cold wave                             2
Heat wave                             2
Extra-tropical storm                  1
Landslide                             1
Lava flow                             1
Mudslide                              1
Name: Disaster Subtype, dtype: int64
# Flood DataFrame
flood_df = usa_df.loc[(df['Disaster Subtype'] == 'Flash flood'),:]
flood_df.head()
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
700	Flood	Flash flood	Natural	Hydrological	NaN	Brief torrential rain	United States of America (the)	New York province	42.2300	-74.95	2007-06-19	2007-06-20	4.0	120.0	NaN	NaN	81.101659
3202	Flood	Flash flood	Natural	Hydrological	NaN	Heavy rains, strong storm system	United States of America (the)	Orange, Mariposa, Tuolumne, Los Angeles, San F...	NaN	NaN	2014-12-02	2014-12-05	NaN	NaN	NaN	90000.0	92.598981
3291	Flood	Flash flood	Natural	Hydrological	NaN	Heavy rain	United States of America (the)	Texas, Oklahoma, Colorado, Arkansas, Kansas, L...	34.1936	-97.99	2015-05-23	2015-05-30	32.0	12000.0	NaN	2700000.0	92.708822
3342	Flood	Flash flood	Natural	Hydrological	NaN	Torrential rains	United States of America (the)	San Marcos area (Hays district, Texas province...	NaN	NaN	2015-05-24	2015-05-24	16.0	12000.0	NaN	NaN	92.708822
3343	Flood	Flash flood	Natural	Hydrological	NaN	Torrentila rains	United States of America (the)	Hildale town (Washington district, Utah provin...	NaN	NaN	2015-09-14	2015-09-14	19.0	NaN	NaN	2000.0	92.708822
# Finding the top deadliest Flood in US (timeframe)
flood_df.sort_values(by=['Total Deaths'], ascending=False).head(1)
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
3291	Flood	Flash flood	Natural	Hydrological	NaN	Heavy rain	United States of America (the)	Texas, Oklahoma, Colorado, Arkansas, Kansas, L...	34.1936	-97.99	2015-05-23	2015-05-30	32.0	12000.0	NaN	2700000.0	92.708822
# Wildfire Dataframe
wildfire_df = usa_df.loc[(df['Disaster Subtype'] == 'Forest fire'),:]
wildfire_df.head()
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
750	Wildfire	Forest fire	Natural	Climatological	NaN	NaN	United States of America (the)	Alpine, Amador, Calaveras, El Dorado, Mono, Pl...	NaN	NaN	2007-06-24	2007-07-02	NaN	NaN	NaN	NaN	81.101659
751	Wildfire	Forest fire	Natural	Climatological	NaN	NaN	United States of America (the)	Malibu area (Los Angeles district, California ...	NaN	NaN	2007-11-24	2007-11-27	NaN	10000.0	NaN	315000.0	81.101659
846	Wildfire	Forest fire	Natural	Climatological	NaN	Lightening, drought, wind	United States of America (the)	California province	NaN	NaN	2008-06-20	2008-07-09	1.0	NaN	NaN	102000.0	84.215229
1639	Wildfire	Forest fire	Natural	Climatological	NaN	Drought, high temperatures and strong winds	United States of America (the)	Bastrop district (Texas province)	NaN	NaN	2011-09-04	2011-09-09	4.0	NaN	NaN	1000000.0	87.984603
2209	Wildfire	Forest fire	Natural	Climatological	NaN	Drought conditions and strong winds	United States of America (the)	Arizona, Minnesota, Texas, Florida provinces	NaN	NaN	2011-05-29	2011-06-23	2.0	NaN	NaN	200000.0	87.984603
# Finding the deadliest wildfire in US (timeframe)
wildfire_df.sort_values(by=['Total Deaths'], ascending=False).head(1)
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
4396	Wildfire	Forest fire	Natural	Climatological	Camp Fire	NaN	United States of America (the)	Butte county (North California)	NaN	NaN	2018-11-08	2018-11-16	88.0	250000.0	NaN	16500000.0	98.219991
# Hurricane DataFrame
tropical_cyclone_df = usa_df.loc[(df['Disaster Subtype'] == 'Tropical cyclone'),:]
tropical_cyclone_df.head()
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
712	Storm	Tropical cyclone	Natural	Meteorological	Erin	NaN	United States of America (the)	Texas, Oklahoma, Missouri provinces	NaN	NaN	2007-08-16	2007-08-19	7.0	NaN	NaN	NaN	81.101659
775	Storm	Tropical cyclone	Natural	Meteorological	Hurricane Dolly	NaN	United States of America (the)	Panama city beach (Bay district, Florida provi...	NaN	NaN	2008-07-23	2008-07-23	NaN	NaN	NaN	1200000.0	84.215229
795	Storm	Tropical cyclone	Natural	Meteorological	Tropical Storm "Fay"	NaN	United States of America (the)	Florida, Georgia, Alabama, Mississippi provinces	NaN	NaN	2008-08-20	2008-08-28	12.0	400.0	NaN	180000.0	84.215229
796	Storm	Tropical cyclone	Natural	Meteorological	Hurricane Ike	NaN	United States of America (the)	Galveston, Brazoria, Harris, Chambers, Jeffers...	35.05	-90.45	2008-09-12	2008-09-16	82.0	200000.0	NaN	30000000.0	84.215229
844	Storm	Tropical cyclone	Natural	Meteorological	Hurricane "Gustav"	NaN	United States of America (the)	Louisiana, Mississippi, Texas, Alabama provinces	NaN	NaN	2008-09-01	2008-09-01	43.0	2100000.0	NaN	7000000.0	84.215229
# Finding the top deadliest Hurricanes in US (timeframe)
tropical_cyclone_df.sort_values(by=['Total Deaths'], ascending=False).head(1)
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
4306	Storm	Tropical cyclone	Natural	Meteorological	Hurricane Harvey	NaN	United States of America (the)	Eastern Texas (Rockport, Corpus Chrsti, Port L...	NaN	NaN	2017-08-25	2017-08-29	88.0	555000.0	NaN	95000000.0	95.878166
# Create initial variables for tropical, wildfire, and flood dataframes
topcyclone = tropical_cyclone_df.loc[(tropical_cyclone_df["Total Deaths"] == tropical_cyclone_df["Total Deaths"].max()),:]
topwildfire = wildfire_df.loc[(wildfire_df["Total Deaths"] == wildfire_df["Total Deaths"].max()),:]
topflood = flood_df.loc[(flood_df["Total Deaths"] == flood_df["Total Deaths"].max()),:]

frames = [topcyclone,topwildfire,topflood]
top_disasters = pd.concat(frames)
top_disasters
Disaster Type	Disaster Subtype	Disaster Group	Disaster Subgroup	Event Name	Origin	Country	Location	Latitude	Longitude	start_date	end_date	Total Deaths	No Affected	Reconstruction Costs ('000 US$)	Total Damages ('000 US$)	CPI
4306	Storm	Tropical cyclone	Natural	Meteorological	Hurricane Harvey	NaN	United States of America (the)	Eastern Texas (Rockport, Corpus Chrsti, Port L...	NaN	NaN	2017-08-25	2017-08-29	88.0	555000.0	NaN	95000000.0	95.878166
4396	Wildfire	Forest fire	Natural	Climatological	Camp Fire	NaN	United States of America (the)	Butte county (North California)	NaN	NaN	2018-11-08	2018-11-16	88.0	250000.0	NaN	16500000.0	98.219991
3291	Flood	Flash flood	Natural	Hydrological	NaN	Heavy rain	United States of America (the)	Texas, Oklahoma, Colorado, Arkansas, Kansas, L...	34.1936	-97.99	2015-05-23	2015-05-30	32.0	12000.0	NaN	2700000.0	92.708822
# Changing data type of top_disasters
top_disasters["start_date"] = top_disasters["start_date"].astype("datetime64")
top_disasters["end_date"] = top_disasters["end_date"].astype("datetime64")
Finding Dates Two Weeks Before and After the Disasters Occur
startdate = (top_disasters["start_date"] - pd.to_timedelta(14,unit = "d")).reset_index(drop = True)
enddate = (top_disasters["end_date"] + pd.to_timedelta(14,unit = "d")).reset_index(drop = True)
print(startdate,enddate)
0   2017-08-11
1   2018-10-25
2   2015-05-09
Name: start_date, dtype: datetime64[ns] 0   2017-09-12
1   2018-11-30
2   2015-06-13
Name: end_date, dtype: datetime64[ns]
Pulling Twitter Data
# Pulling tweet data
samplefile = 'Resources/NA_sample_020.csv'
df2 = pd.read_csv(samplefile)
df2.head()
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness
0	8037902	2018-01-31 20:40:44+00:00	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive
1	8790126	2018-03-15 21:28:05+00:00	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive
2	11647260	2018-09-29 14:42:56+00:00	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive
3	11810796	2018-10-08 22:23:50+00:00	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive
4	13183104	2018-12-16 11:52:40+00:00	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive
# Changing data type to datetime
df2["created_at"] = df2["created_at"].astype("datetime64")
# Adding date column
df2['date'] = df2['created_at'].dt.date
df2["date"] = df2["date"].astype("datetime64")
df2["date"] = df2["date"].dt.date
df2.head()
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
0	8037902	2018-01-31 20:40:44	40.441694	-79.990086	-0.120697	Politics	believer	male	aggressive	2018-01-31
1	8790126	2018-03-15 21:28:05	38.895110	-77.036370	-0.378573	Global stance	believer	male	not aggressive	2018-03-15
2	11647260	2018-09-29 14:42:56	44.000340	-72.749830	-0.216689	Politics	believer	female	not aggressive	2018-09-29
3	11810796	2018-10-08 22:23:50	39.952330	-75.163790	-0.687731	Global stance	believer	female	not aggressive	2018-10-08
4	13183104	2018-12-16 11:52:40	45.421106	-75.690308	0.611834	Weather Extremes	believer	male	not aggressive	2018-12-16
Finding Tweets two Weeks Before/After Each Disaster
# Create variables for start and end times for tweets
cyclone_tweets = df2.loc[(df2["date"] >= startdate[0]) & (df2["date"] <= enddate[0]), :]
wildfire_tweets = df2.loc[(df2["date"] >= startdate[1]) & (df2["date"] <= enddate[1]), :]
flood_tweets = df2.loc[(df2["date"] >= startdate[2]) & (df2["date"] <= enddate[2]), :]
c:\Users\barim\anaconda3\envs\PythonData\lib\site-packages\pandas\core\ops\array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.  In a future version these will be considered non-comparable.Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.
  result = libops.scalar_compare(x.ravel(), y, op)
# Displaying results for hurricane
print(startdate[0],enddate[0])
cyclone_tweets.head()
2017-08-11 00:00:00 2017-09-12 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
216	4992854	2017-08-15 17:30:04	38.419250	-82.445154	-0.465921	Global stance	believer	male	not aggressive	2017-08-15
360	5026051	2017-09-10 22:11:53	36.030113	-114.982619	-0.534735	Donald Trump versus Science	believer	male	aggressive	2017-09-10
640	5028248	2017-09-11 02:43:21	29.285810	-81.055890	-0.524462	Global stance	believer	male	not aggressive	2017-09-11
1072	5028195	2017-09-11 02:35:47	43.038900	-87.906470	-0.589799	Ideological Positions on Global Warming	believer	male	aggressive	2017-09-11
1219	5039910	2017-09-12 03:08:16	45.713952	-122.706927	-0.535728	Politics	believer	female	not aggressive	2017-09-12
# Displaying results for wildfire
print(startdate[1],enddate[1])
wildfire_tweets.head()
2018-10-25 00:00:00 2018-11-30 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
108	12184635	2018-11-26 16:55:15	44.108850	-79.122677	-0.028878	Global stance	neutral	male	aggressive	2018-11-26
158	12380621	2018-11-28 16:04:45	34.053691	-118.242766	-0.593402	Global stance	believer	female	not aggressive	2018-11-28
290	12478425	2018-11-30 03:55:32	30.271129	-97.743700	-0.638252	Importance of Human Intervantion	believer	male	not aggressive	2018-11-30
292	12505618	2018-11-30 14:53:32	40.723710	-73.704850	0.715171	Weather Extremes	neutral	female	not aggressive	2018-11-30
320	12447850	2018-11-29 17:19:54	34.052230	-118.243680	0.002881	Donald Trump versus Science	denier	male	not aggressive	2018-11-29
# Displaying results for flood
print(startdate[2],enddate[2])
flood_tweets.head()
2015-05-09 00:00:00 2015-06-13 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
101	2590945	2015-05-28 20:48:13	28.750540	-82.500100	0.155581	Politics	neutral	male	not aggressive	2015-05-28
270	2673100	2015-06-12 04:48:15	42.527412	-92.445318	0.494693	Weather Extremes	believer	male	not aggressive	2015-06-12
308	2590530	2015-05-28 19:45:17	38.895037	-77.036543	-0.523694	Weather Extremes	neutral	female	aggressive	2015-05-28
589	2667006	2015-06-11 02:05:12	53.550140	-113.468710	0.459667	Global stance	believer	female	not aggressive	2015-06-11
608	2584784	2015-05-28 07:28:08	35.084103	-106.650985	-0.514281	Politics	neutral	male	not aggressive	2015-05-28
Finding Tweets Based on 'Weather Extreme' Topic
# cyclone tweets Weather Extreme topic data
print(startdate[0],enddate[0])
cyclone_we = cyclone_tweets.loc[cyclone_tweets['topic'] == "Weather Extremes"]
cyclone_we.head()
2017-08-11 00:00:00 2017-09-12 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
2459	4995252	2017-08-17 20:10:01	39.290882	-76.610759	0.512937	Weather Extremes	neutral	undefined	not aggressive	2017-08-17
2882	4994199	2017-08-16 03:17:02	40.348720	-74.659050	-0.144761	Weather Extremes	believer	female	not aggressive	2017-08-16
3059	5004533	2017-08-28 06:30:21	41.921673	-93.312270	0.402846	Weather Extremes	neutral	undefined	not aggressive	2017-08-28
4890	4997020	2017-08-21 04:22:17	28.542111	-81.379030	-0.317874	Weather Extremes	neutral	male	not aggressive	2017-08-21
6053	5000311	2017-08-25 03:33:48	41.262128	-95.861391	-0.092279	Weather Extremes	denier	male	aggressive	2017-08-25
# Wildfire tweets Weather Extreme topic data
print(startdate[1],enddate[1])
wildfire_we = wildfire_tweets.loc[wildfire_tweets['topic'] == "Weather Extremes"]
wildfire_we.head()
2018-10-25 00:00:00 2018-11-30 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
292	12505618	2018-11-30 14:53:32	40.723710	-73.704850	0.715171	Weather Extremes	neutral	female	not aggressive	2018-11-30
1033	12254231	2018-11-27 03:42:39	26.640628	-81.872308	-0.780085	Weather Extremes	believer	male	not aggressive	2018-11-27
1427	12232710	2018-11-26 23:56:56	34.052230	-118.243680	0.136009	Weather Extremes	believer	male	aggressive	2018-11-26
2025	12354629	2018-11-28 06:17:51	32.750420	-83.500180	-0.379218	Weather Extremes	believer	male	aggressive	2018-11-28
2205	12336341	2018-11-28 01:15:45	29.424600	-98.495141	0.005411	Weather Extremes	believer	male	aggressive	2018-11-28
# Flood tweets Weather Extreme topic data
print(startdate[2],enddate[2])
flood_we = flood_tweets.loc[flood_tweets['topic'] == "Weather Extremes"]
flood_we.head()
2015-05-09 00:00:00 2015-06-13 00:00:00
Unnamed: 0	created_at	lat	lng	sentiment	topic	stance	gender	aggressiveness	date
270	2673100	2015-06-12 04:48:15	42.527412	-92.445318	0.494693	Weather Extremes	believer	male	not aggressive	2015-06-12
308	2590530	2015-05-28 19:45:17	38.895037	-77.036543	-0.523694	Weather Extremes	neutral	female	aggressive	2015-05-28
1525	2671310	2015-06-11 19:05:18	35.084103	-106.650985	0.613703	Weather Extremes	neutral	male	not aggressive	2015-06-11
3129	2554467	2015-05-18 20:26:03	40.800000	-96.666960	0.039903	Weather Extremes	neutral	male	not aggressive	2015-05-18
3275	2666709	2015-06-11 00:31:18	44.500240	-90.000410	0.061085	Weather Extremes	believer	male	not aggressive	2015-06-11
Graphs of Tweet Activity for Top Disasters
# Hurricane Harvey tweets 2 weeks before/after
new_tc_df = cyclone_tweets.groupby("date").count()
tropical_period = new_tc_df.loc[:,"sentiment"]

tropical_period.plot(kind="bar", figsize=(8,5),color='cyan')
plt.xlabel("Date of Tweets")
plt.ylabel("Number of Tweets")
plt.title("Hurricane Harvey Tweet Activity")

plt.axvline(14, color='red', linestyle=":")
plt.axvline(18, color='red', linestyle=":")
plt.text(13.5, 150, 'Hurricane Harvey Landfall', color='purple', rotation=90)
plt.text(17.5, 150, 'Hurricane Harvey Ends', color='purple', rotation=90)

plt.tight_layout()
plt.savefig('Images/Q3Hurricane.png', transparent = False, facecolor= "white")
plt.show()

# CA wildfire 2 weeks before/after
new_wf_df = wildfire_tweets.groupby("date").count()
wildfire_period = new_wf_df.loc[:,"sentiment"]

wildfire_period.plot(kind="bar", figsize=(8,5), color='orange')
plt.xlabel("Date of Tweets")
plt.ylabel("Number of Tweets")
plt.title("2018 California Wildfires Tweet Activity")

plt.axvline(.25, color='red', linestyle=":")
plt.axvline(.75, color='red', linestyle=":")
plt.text(.10, 1100, 'CA Wildfire Begins', color='purple', rotation=90)
plt.text(.63, 1100, 'CA Wildfire Ends', color='purple', rotation=90)

plt.tight_layout()
plt.savefig('Images/Q3Wildfire.png', transparent = False, facecolor= "white")
plt.show()

# Wimberley Flood 2 weeks before/after
new_flood_df = flood_tweets.groupby("date").count()
flood_period = new_flood_df.loc[:,"sentiment"]

flood_period.plot(kind="bar", figsize=(10,5))
plt.xlabel("Date of Tweets")
plt.ylabel("Number of Tweets")
plt.title("Wimberley Flood Tweet Activity")

plt.axvline(14, color='red', linestyle=":")
plt.axvline(21, color='red', linestyle=":")
plt.text(13.25, 250, 'Wimberley Flood Begins', color='purple', rotation=90)
plt.text(20.25, 300, 'Wimberley Flood Ends', color='purple', rotation=90)

plt.tight_layout()
plt.savefig('Images/Q3Flood.png', transparent = False, facecolor= "white")
plt.show()

Analysis:
Hurrican Harvey: There is a significant increase of tweets after Hurricane Harvey concluded. During the storm there was an average amount of activity based on all the topics of the data that include climate change. When we break down the graph to just include the topic of 'Weather Extremes' there is an increase in acivity throughout the duration of the storm. There is an upward trend of weather and climate related tweets even two weeks after Hurricane Harvey concludes. As we recall, Hurricane Harvey was one of the more devestating hurricanes in recent memory which continues to show how these storms continue to get stronger over time.

2018 California Wildfires: Looking at the data of the 2018 California Wildfire tweet activity it shows a trend up after the wildfires had concluded. Considering we really don't know the severity of a wildfire until it is contained or it has concluded, the tweet data two weeks before the wildfire begins, there is minimum tweet activity across all weather and climate topics on Twitter. It's not until the Wildfire concludes that there is a trend up in tweet acitivity acorss all climate related tweets.

2015 Wimberley Flood: The 2015 Wimberley Floods started on May 23, 2015. The data shows that there is a uptick of tweets as the storms and flood trended up as the days went on and the flood began to affect more people. On May 28, there is a significant uptick in tweet activity. Two weeks before the flood occured, there was a decent amount of tweets related to weather and cliumate topics on twitter, with a slight deep days before the flood begins.
